{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 豆瓣TOP250短评数据建立情感分析器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 读取文件中的评论和相应的用户评分\n",
    "* 将用户评分转换为数值型\n",
    "* 取出评分不等于3的评论和评分\n",
    "* 将低于3分的标记为0(不喜欢)，高于3分的标记为1(喜欢)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>user_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>我们大天朝每天都在发生这样的事儿，类似的纪录片可以看到很多，只是没进入大屏幕而已啦。说实话，...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>无恶意…只是本人对记录片真心无爱…看了75分钟算是我看的时间最久的纪录片了！</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>其实我觉得这部拍得不错了。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>时间走过70年的2013年元旦， 我第一次看了这部如雷贯耳的经典老片。 作为经典， 它当之无...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>更爱库布里克袅。难以形容的愉悦与快感，一种脑壳难受三观颠倒的快感，一种投身邪恶告别俗人的愉悦...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  user_score\n",
       "1  我们大天朝每天都在发生这样的事儿，类似的纪录片可以看到很多，只是没进入大屏幕而已啦。说实话，...           0\n",
       "2             无恶意…只是本人对记录片真心无爱…看了75分钟算是我看的时间最久的纪录片了！           0\n",
       "4                                      其实我觉得这部拍得不错了。           1\n",
       "5  时间走过70年的2013年元旦， 我第一次看了这部如雷贯耳的经典老片。 作为经典， 它当之无...           1\n",
       "6  更爱库布里克袅。难以形容的愉悦与快感，一种脑壳难受三观颠倒的快感，一种投身邪恶告别俗人的愉悦...           1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取文件短评内容\n",
    "with open('clean_data.csv', 'r') as f:\n",
    "    comment_data = f.read()\n",
    "comment_data = comment_data.split('\\n') # 以换行符分割短评\n",
    "\n",
    "# 用短评和用户评分创建DataFrame\n",
    "comment = []\n",
    "user_score = []\n",
    "for item in comment_data:\n",
    "    s = re.search(r'(\\w+,)(https://www.douban.com/people/.+/,)(.*,)(\\d,)(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},)(.+)(,\\d+)', item)\n",
    "    if s:\n",
    "        comment.append(s.group(6)) # 取出短评信息中的评论\n",
    "        user_score.append(s.group(4)[:-1]) # 取出短评信息中的用户评分\n",
    "comments = pd.DataFrame({'comment':comment,\n",
    "                        'user_score':user_score})\n",
    "\n",
    "# 将低于3分的标记为0(不喜欢)，高于3分的标记为1(喜欢)\n",
    "def comment_type(score):\n",
    "    if score > 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "comments['user_score'] = pd.to_numeric(comments['user_score']) # 转换成数值型\n",
    "# 取出用户评分不为3的数据\n",
    "comments_for_model = comments.loc[comments['user_score'] != 3].copy()\n",
    "comments_for_model['user_score'] = comments_for_model['user_score'].apply(comment_type)\n",
    "# 查看数据前5行\n",
    "comments_for_model.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "去除评论中的数字和字母，并去掉全为英文的评论，只保留中文的评论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>user_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>我们大天朝每天都在发生这样的事儿，类似的纪录片可以看到很多，只是没进入大屏幕而已啦。说实话，...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>无恶意…只是本人对记录片真心无爱…看了分钟算是我看的时间最久的纪录片了！</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>其实我觉得这部拍得不错了。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>时间走过年的年元旦， 我第一次看了这部如雷贯耳的经典老片。 作为经典， 它当之无愧， 算作新...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>更爱库布里克袅。难以形容的愉悦与快感，一种脑壳难受三观颠倒的快感，一种投身邪恶告别俗人的愉悦...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  user_score\n",
       "1  我们大天朝每天都在发生这样的事儿，类似的纪录片可以看到很多，只是没进入大屏幕而已啦。说实话，...           0\n",
       "2               无恶意…只是本人对记录片真心无爱…看了分钟算是我看的时间最久的纪录片了！           0\n",
       "4                                      其实我觉得这部拍得不错了。           1\n",
       "5  时间走过年的年元旦， 我第一次看了这部如雷贯耳的经典老片。 作为经典， 它当之无愧， 算作新...           1\n",
       "6  更爱库布里克袅。难以形容的愉悦与快感，一种脑壳难受三观颠倒的快感，一种投身邪恶告别俗人的愉悦...           1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_comment = comments_for_model['comment'].str.replace(r'[a-zA-Z0-9]', '') # 去掉评论中的字母和数字\n",
    "cn_comments = clean_comment.str.contains(r'\\w') # 中文评论的索引\n",
    "model_data = comments_for_model[cn_comments].copy() # 取出中文评论\n",
    "model_data['comment'] = model_data['comment'].str.replace(r'[a-zA-Z0-9]', '') # 去掉评论中的字母和数字\n",
    "model_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.331 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    135104\n",
       "0     88277\n",
       "Name: user_score, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对评论进行分词\n",
    "model_data['comment'] = model_data['comment'].apply(lambda x: ' '.join(jieba.cut(x)))\n",
    "\n",
    "X = model_data['comment']\n",
    "y = model_data['user_score']\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到表示喜欢的评论多于表示不喜欢的评论"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 读取停用词表\n",
    "stopwords = []\n",
    "with open('stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    for stopword in f.readlines():\n",
    "        stopwords.append(stopword[:-1]) # 去掉换行符\n",
    "stopwords[0] = stopwords[0][-1] # 去除第一个字符中的无用字符\n",
    "stopwords = stopwords + ['一个', '这部', '一部', '片子', '这是', '还有', '应该']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 将数据集分割为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 朴素贝叶斯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79164129928732585"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# 抽取1-gram和2-gram的统计特征\n",
    "vec = CountVectorizer(analyzer='word', ngram_range=(1, 2), max_features=8000, stop_words=stopwords)\n",
    "vec.fit(X_train)\n",
    "classifier = MultinomialNB() # 创建实例\n",
    "classifier.fit(vec.transform(X_train), y_train) # 拟合数据\n",
    "classifier.score(vec.transform(X_test), y_test) # 模型准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15188  4725]\n",
      " [ 6911 29022]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.76      0.72     19913\n",
      "          1       0.86      0.81      0.83     35933\n",
      "\n",
      "avg / total       0.80      0.79      0.79     55846\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "# 预测测试集\n",
    "y_predict = classifier.predict(vec.transform(X_test))\n",
    "print (confusion_matrix(y_predict, y_test)) # 输出混淆矩阵\n",
    "print (classification_report(y_predict, y_test)) # 输出其它评价指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逻辑回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76044837589084269"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 抽取1-gram和2-gram的统计特征\n",
    "vec = CountVectorizer(analyzer='word', ngram_range=(1, 2), max_features=1000, stop_words=stopwords)\n",
    "vec.fit(X_train)\n",
    "\n",
    "lr_clf = LogisticRegression() # 创建实例\n",
    "lr_clf.fit(vec.transform(X_train), y_train) # 拟合数据\n",
    "lr_clf.score(vec.transform(X_test), y_test) # 模型准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对模型进行网格搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 网格搜索的参数\n",
    "param_grid = {'penalty':['l1', 'l2'],\n",
    "             'C':[0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "best_score = 0\n",
    "# 用不同特征数进行网格搜索\n",
    "for features in range(1000, 5500, 500):\n",
    "    # 抽取1-gram和2-gram的统计特征\n",
    "    vec = CountVectorizer(analyzer='word', ngram_range=(1, 2), max_features=features, stop_words=stopwords)\n",
    "    vec.fit(X_train)\n",
    "    # 进行网格搜索\n",
    "    grid_search = GridSearchCV(LogisticRegression(), param_grid=param_grid, cv=5).fit(vec.transform(X_train), y_train)\n",
    "    estimator = grid_search.best_estimator_\n",
    "    score = grid_search.best_score_\n",
    "    params = grid_search.best_params_\n",
    "    # 取出准确路最佳的模型\n",
    "    if score > best_score:\n",
    "        best_estimator = estimator\n",
    "        best_score = score\n",
    "        best_params = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "0.799164353717\n",
      "{'penalty': 'l2', 'C': 0.1}\n",
      "[[14373  3296]\n",
      " [ 7726 30451]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.81      0.72     17669\n",
      "          1       0.90      0.80      0.85     38177\n",
      "\n",
      "avg / total       0.82      0.80      0.81     55846\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (best_estimator) # 输出最佳模型\n",
    "print (best_score) # 输出最佳准确率\n",
    "print (best_params) # 输出最佳模型的参数\n",
    "# 预测测试集\n",
    "y_predict = best_estimator.predict(vec.transform(X_test))\n",
    "print (confusion_matrix(y_predict, y_test)) # 输出混淆矩阵\n",
    "print (classification_report(y_predict, y_test)) # 输出其它评价指标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用网格搜索得到的最佳模型的准确率略高于朴素贝叶斯，但是使用特征数少于朴素贝叶斯所使用的特征数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用TfidfVectorizer提取文本特征并用朴素贝叶斯建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79916198116248249"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import  TfidfVectorizer\n",
    "# 抽取1-gram和2-gram的统计特征\n",
    "tf = TfidfVectorizer(analyzer='word',\n",
    "                    ngram_range=(1, 2),\n",
    "                     max_features=8000,\n",
    "                    stop_words=stopwords)\n",
    "tf.fit(X_train)\n",
    "# 建模\n",
    "tf_clf = MultinomialNB()\n",
    "tf_clf.fit(tf.transform(X_train), y_train)\n",
    "tf_clf.score(tf.transform(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13768  2885]\n",
      " [ 8331 30862]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.83      0.71     16653\n",
      "          1       0.91      0.79      0.85     39193\n",
      "\n",
      "avg / total       0.83      0.80      0.81     55846\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_predict = tf_clf.predict(tf.transform(X_test))\n",
    "print (confusion_matrix(y_predict, y_test))\n",
    "print (classification_report(y_predict, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
